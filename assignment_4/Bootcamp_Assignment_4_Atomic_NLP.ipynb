{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bootcamp Assignment 4: Atomic NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Congratulations on Completing Assignment 3ðŸŽ‰ðŸŽ‰\n",
        "\n",
        "Hi Everyone! Hope you all had fun solving Assignment 3 and hopefully you learnt something new from it. Next up, we have our final assignment of this bootcamp on the topic **Natural Language Processing**. Can we have some hoots again ladies and gentlemen? \n",
        "\n",
        "In this assignment, we'll restrict ourselves to text classification only. We'll guide you through the following topics:-\n",
        "\n",
        "* Basics of Text Pre-Processing\n",
        "    * Regex\n",
        "    * Stemming and Lemmetization\n",
        "    * Tokenization\n",
        "    * Stopwords\n",
        "    * Frequency Based Embeddings\n",
        "* Text Classification\n",
        "* Hyperparameter Tuning using Optuna [EXTRA]"
      ],
      "metadata": {
        "id": "l2sQG-CbqBeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "NLP is probably the most hyped domain in the past year, we saw transformers rise to fame and saw libraries like **Hugging Face** help us easily use them. But before the dawn of Transformers things were bit different. Not everyone used these heavy af models and to be honest not everyone had the resources too. So let's go back to the basics and see how these things worked out.\n",
        "\n",
        "![](https://i.imgur.com/Lyf9O95.jpeg)\n",
        "\n",
        "Before getting to the approach we'll need to understand how to pre-process the data that we'll get. In NLP, most of the time what you'll get would be pieces of raw text and well that's no good for us. What we need to do is to take this data and transform it to something that our model understands i.e. Numbers.\n",
        "\n",
        "![](https://i.ibb.co/KhxS04m/image-blah.jpg)\n",
        "\n",
        "With that said let's go to our first topic of interest and the soul of NLP i.e. **Regex**."
      ],
      "metadata": {
        "id": "la4ABeg4r_de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regex\n",
        "\n",
        "Regular Expression or as everyone says **Regex** is an alien language that helps us detect pattern in the string. While that statement might seem exaggerated to those who know regex, newbies can relate to the feeling of looking at a regex pattern and screaming WHAT TH...\n",
        "\n",
        "Pardon the language, while that's true it's actually not that hard to figure out regex. Let's see a few important regex tokens and see what purpose they serve.\n",
        "\n",
        "* **\\d:** Match Digits\n",
        "* **\\s:** Match Whitespaces\n",
        "* **\\w:** Match Alphabets and Whitespaces\n",
        "* **\\D:** Match Everything except Digits\n",
        "* **\\S:** Match Everything except whitespaces.\n",
        "* **\\W:** Match Everything except Alphabets and Whitespaces\n",
        "* **. :** Yep thats a dot, it's used to match anything\n",
        "* **+:** Match 1 or more occurance of something\n",
        "* ***:** Match 0 or more occurance of something\n",
        "\n",
        "To understand how to use regex and how it can be used you can refer [**this article**](https://mlwhiz.com/blog/2019/09/01/regex/) or [**this video**](https://www.youtube.com/watch?v=K8L6KVGG-7o). Once you are done, you can get your hands dirty with the following questions:-"
      ],
      "metadata": {
        "id": "eZm7jp2AueSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Extract the domain name from these simple urls, which always start with `http` and end with `.com`.**\n",
        "\n",
        " **HINT:** the `match any character` metacharacter will be very helpful here. \n",
        "```\n",
        "input: 'https://www.kaggle.com https://www.google.com https://www.wikipedia.com'\n",
        "```"
      ],
      "metadata": {
        "id": "q9Tfmso0yZMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "JSGlJv2BsJef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Write a pattern that will return numbers with no zeroes.**\n",
        "\n",
        "```\n",
        "input: '123, 012410, 01010, , 000, 111, 3495873, 3, not a number!, ...!@$,.'\n",
        "```"
      ],
      "metadata": {
        "id": "C3LooZJRGIdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "SR_uA02FGIMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Write a pattern to count the number of sentences that end with a word ending in 'ing' or 'ings'.**\n",
        "\n",
        " **HINT:** if you find that you're matching more items than expected, try a regex tester like [Pythex](pythex.org) so you can visualize exactly what's going wrong. \n",
        "```\n",
        "input: 'Looking for many endings? You should only be seeing one match.'\n",
        "```"
      ],
      "metadata": {
        "id": "64bgTVu1GPyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "_6G-mYLXGPip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Count the number of words in this sentence with at least five characters.**\n",
        "```\n",
        "input: 'Count the number of words in this sentence with at least five characters.'\n",
        "```"
      ],
      "metadata": {
        "id": "CSS41ExDGVVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "U-UC8YwOGZlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Extract the two phone numbers from this sentence.**\n",
        "```\n",
        "input: 'Extract these two normally formatted phone numbers from this sentence: (123) 456 7890, 123-456-7890.'\n",
        "```"
      ],
      "metadata": {
        "id": "-M-WzYvPGa2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "OlvwaZEIGdza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Write a pattern to extract the domain name from an email address in a string. For this sentence, notreal@notmail.com should return 'notmail'.**\n",
        "```\n",
        "input: 'An email address (imaginaryperson@imaginarymail.edu) in a sentence. Don't match Invalid_email@invalid.'\n",
        "```"
      ],
      "metadata": {
        "id": "fRMrxxAhGeLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "a37bhOS4GhPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Identify all of words that look like names in the sentence. In other words, those which are capitalized but aren't the first word in the sentence.**\n",
        "```\n",
        "input: 'This is not a name, but Harry is. So is Susy. Sam should be missed as it's the first word in the sentence.'\n",
        "```"
      ],
      "metadata": {
        "id": "A8t3FpF-GhiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "pNb0BAvjGkNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "When you speak sentences you speak them by structuring a word in a way that conveys a meaning. Most of these \"structuring\" help out in describing the grammatical structure of the sentence to convey a specific meaning. For example, \"go\" and \"going\" are two different words that capture the essence of same event i.e. event of going somewhere. \n",
        "\n",
        "To us these grammatical structure matter but to our ML model it's all the same. So most of the times what we do is to convert these words back to their root form. For example:-\n",
        "\n",
        "* going -> go\n",
        "* travelling -> travel\n",
        "* Omaiwamou shindeiru -> nani\n",
        "\n",
        "Well maybe not the last one but you get the gist right? **Stemming** and **Lemmatization** both convert the word to their root form. The difference between them is that the root form found by lemmatization is always a meaningful word that'll be a part of english vocabulary, while stemming can result in getting root forms that may or maynot be a meaningful word.\n",
        "\n",
        "To understand more about stemming and lemmatization you can refer [**this video**](https://www.youtube.com/watch?v=OQxi-d5C9j8) by **Abhishek Thakur**. "
      ],
      "metadata": {
        "id": "lYyUidwvyew_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Apply PorterStemmer on the following sentences:-**\n",
        "```\n",
        "sentence 1: All sentences have a noun or pronoun component called the subject, and a verb part called the predicate. \n",
        "sentence 2: Although he organized his sources by theme, he decided to arrange them chronologically, and he carefully followed the MEAL plan for organization. \n",
        "```"
      ],
      "metadata": {
        "id": "FDCGAoda2MK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "B1LILI2-2JY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Apply WordnetLemmatizer on the following sentences:-**\n",
        "```\n",
        "sentence 1: All sentences have a noun or pronoun component called the subject, and a verb part called the predicate. \n",
        "sentence 2: Although he organized his sources by theme, he decided to arrange them chronologically, and he carefully followed the MEAL plan for organization. \n",
        "```"
      ],
      "metadata": {
        "id": "xh2efyHgH1Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "yJKCbIL5Hzl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords and Tokenization\n",
        "\n",
        "Stopwords are basically words that frequently occur in most of the sentences and because of that they might not be very helpful in distinguishing the characteristic of the sentence, hence we remove them most of the time. Now in order to do this you need to check whether a word is a stopword or not. And before that, we need to extract all the words. For that we use tokenizations which means splitting sentence into a list of words that we call \"tokens\".\n",
        "\n",
        "**Stopwords** can be fetched using NLTK which provides list of stopwords for various languages.\n",
        "\n",
        "**Tokenization** can be done either using **split()** method or **word_tokenize()** method in NLTK. "
      ],
      "metadata": {
        "id": "gQ0Z9O0S2Spl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Remove stopwords from the following sentences:-**\n",
        "```\n",
        "sentence 1: Susie works in a shoeshine shop. Where she shines she sits, and where she sits she shines. \n",
        "sentence 2: Manners thus play a significant role in a man's life, without which he can never be, a complete man in the true sense of the word. This has been apty summed up when we say, \"Manners Maketh Man\". \n",
        "```"
      ],
      "metadata": {
        "id": "b4BuPORc4tvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "xSCiXqmb4tf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequency Based Embeddings\n",
        "\n",
        "There are many ways with which you can convert sentences into vectors. The one we'll focus on is using **Frequency Based Embeddings**. Which basically means converting the sentence into a vector that represents the frequency of words in some form. There are 2 popular ways we can get these in sklearn:-\n",
        "\n",
        "* **CountVectorizer:** Which converts the sentence in the form of a vector where each vector represents the count of each word in the given sentence.\n",
        "\n",
        "* **TfidfVectorizer:** Which converts the sentence in the form of a vector where each vector represents the tfidf-score of each word in the given sentence. Tfidf score can be calculated using:-\n",
        "\n",
        " ![](https://i.ibb.co/CQRPc4m/image.png)"
      ],
      "metadata": {
        "id": "N-ZbCxTW4xTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Apply `CountVectorizer` and `TfidfVectorizer` on the following sentences:-**\n",
        "\n",
        "```\n",
        "Sentence 1: She completed her literature review, but she still needs to work on her methods section even though she finished her methods course last semester.\n",
        "Sentence 2: Although he organized his sources by theme, he decided to arrange them chronologically, and he carefully followed the MEAL plan for organization. \n",
        "Sentence 3: With pizza and soda at hand, they studied APA rules for many hours, and they decided that writing in APA made sense because it was clear, concise, and objective.\n",
        "```"
      ],
      "metadata": {
        "id": "t84V5E5q9_SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "L52-OKja9_EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification\n",
        "\n",
        "Now that you're familiar with how you can pre-process text to make it into a form that the model understand, we'll dive into text classification on a proper dataset. Your task is simple:-\n",
        "\n",
        "1. Load the `twitter.csv`.\n",
        "2. Pre-process this dataset.\n",
        "3. Apply 3 ML models on the above dataset and compare their performance."
      ],
      "metadata": {
        "id": "w0A9LgdAsCld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "UanPrSaYsJFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using Optuna [EXTRA]\n",
        "\n",
        "In the previous assignment, we used GridSearch to do hyperparameter tuning. In this assignment we'll use a faster and efficient approach for parameter search using Optuna. The library is fairly simple to use however you can go through the [**docs**](https://optuna.org/#code_examples) and [**this video**](https://www.youtube.com/watch?v=5nYqK-HaoKY) to understand better."
      ],
      "metadata": {
        "id": "HBQ0Hm4XsFFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Use Optuna to tune a model of your choice on the above dataset and compare the performance.** "
      ],
      "metadata": {
        "id": "9s0gGAeoBzn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "DsdOciI2sB3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Endnotes\n",
        "\n",
        "Time to get you emotional. This was the last assignment for the bootcamp. We hope you all enjoyed doing it as much as we enjoyed making it. This being the first bootcamp that we organized, not only you were learning but we were learning as well. Hence, if you found any mistakes, deal with it (jkðŸ˜‚). At last we would like to congratulate you for completing the bootcamp and hope you all got to learn a lot. To some it might be standard stuff but these foundations will go a long way. \n",
        "\n",
        ">_May Odin give you knowledge on your path,_\n",
        ">\n",
        ">_May Thor grant you strength and courage on your way,_\n",
        ">\n",
        ">_And May Loki give you laughter as you go._\n",
        "\n",
        "Best,<br>Crework"
      ],
      "metadata": {
        "id": "F0gT8wZ_MZiQ"
      }
    }
  ]
}