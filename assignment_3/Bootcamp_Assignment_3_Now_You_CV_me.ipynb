{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bootcamp Assignment 3 - Now You CV me.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Congratulations on Completing Assignment 2ðŸŽ‰ðŸŽ‰\n",
        "\n",
        "Hi Everyone! Hope you all had fun solving Assignment 2 and hopefully it didn't give you too much trouble ðŸ˜…. Next we'll look at something rather interesting i.e. **Computer Vision**. Can we have some hoots ladies and gentlemen? Computer Vision comprises of various applications like image classification, object detection, saliency detection, etc.\n",
        "\n",
        "In this assignment, we'll restrict ourselves to image classification only. To tell you about the topics we'll be going through the following topics:-\n",
        "\n",
        "* Basics of OpenCV\n",
        "    * How computer percieves images?\n",
        "    * Reading Images with OpenCV\n",
        "* Face Recognition with KNN\n",
        "    * Image Classification with ML Algorithms\n",
        "* Image Classification with SVM\n",
        "    * Feature Extraction\n",
        "    * Dimentionality Reduction\n",
        "    * CIFAR-10"
      ],
      "metadata": {
        "id": "xhfOJp3zDaaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of OpenCV\n",
        "\n",
        "Before diving directly in algorithms it's important to know how to load images to create the dataset. **OpenCV** and **Pillow** are often the 2 libraries of choices whenever the task of loading a dataset comes to mind. While OpenCV is the one that blends well with sklearn and tensorflow, Torchvision's tranformations are aimed at PIL format images. You can still use OpenCV but now you'll have to add an extra transform. \n",
        "\n",
        "Especially with Raspberry Pi when you use Pi Camera, OpenCV blends very well. But what's special about OpenCV? Well OpenCV captures images as numpy array. If you can't understand how then let's take a look at how computers percieve images."
      ],
      "metadata": {
        "id": "N9n_5PwEUSgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How computers percieves images?\n",
        "\n",
        "Images are basically an arrangement of pixels with different **intensity**. Let's take a look at the following image:-\n",
        "\n",
        "![](https://i.ibb.co/wgz8rDf/mnist-mlp-8-0.png)\n",
        "\n",
        "As you can see the above image is of **0**, computer percieves this 28x28 sized image as a matrix with dimension 28x28. Each value representes the **intensity** of white color in the image. These values lie between 0-255(0 is **black** and 255 is **white**), but that's not the case with above image because we normalised the values. But there is one more factor involved here i.e. **channels**.\n",
        "\n",
        "To be honest, channels are a bit hard to describe because even though we can say they represent number on images, that might not be the case everytime. Maybe we can say safely that it carries information required to build a pixel in the given format i.e. greyscale and colored. So let's understand it by cases:-\n",
        "\n",
        "* A greyscale image has **1 channel**. Having one channel means there will only be one matrix representing how **white** that pixel is.\n",
        "* A colored image has **3 channels**, i.e. there will be 3 matrices of shape h x w representing **RGB channels**. Combination of these three give rise to various color combination.\n",
        "\n",
        "That's all there is you need to know about images as of now. Although there can be a **4th alpha channel** representing transparency of the pixel. Commonly used in background removal tasks."
      ],
      "metadata": {
        "id": "5SMOrPFf6O3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Images with OpenCV\n",
        "\n",
        "Reading Image in opencv is as easy as printing something in python. **imread** is the method we use to read images with opencv. What this returns is a numpy array with all channels and pixel values that build up that image. For example:-\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread(img_path)\n",
        "```\n",
        "\n",
        "If you want the colored image to be read as a greyscale image you can pass 0 as a parameter:-\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "\n",
        "colored_img = cv2.imread(img_path)\n",
        "grey_img = cv2.imread(img_path, 0)\n",
        "```\n",
        "\n",
        "There are many operations that you can do with OpenCV and that's what these task will be about."
      ],
      "metadata": {
        "id": "-wsdBiYW_0M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Download and read an image of your choice via OpenCV.**"
      ],
      "metadata": {
        "id": "D1G0oRRFBXnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "igZXVLUVloBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Display the above image in the block below.**\n",
        "\n",
        "**Note:** Although in your local machine you can use **imshow** you can't use it in colab. To display in colab you do it differently, [refer this](https://stackoverflow.com/a/70142998/12089982)."
      ],
      "metadata": {
        "id": "qwSBvDiPBl1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "nm-lmy-ZCpua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Resize the image to half the size using cv2.resize.**"
      ],
      "metadata": {
        "id": "c24e51UFCnuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "yYaLT-0SC3yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Convert the image to HSV format and explain above HSV format in brief.**\n",
        "\n",
        "**Ans.**"
      ],
      "metadata": {
        "id": "id4J12txC6OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "2edY6og4DIol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5[BONUS]. Read a video in OpenCV and explain the code in detail.**\n",
        "\n",
        "**Ans.**"
      ],
      "metadata": {
        "id": "xWmqhrDhDNZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "oAxBC7PwDXlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face Recognition with KNN\n",
        "\n",
        "Well that wasn't that hard, was it? Hopefully not. Anyway, *let's skip to the good part*. We won't directly jump into loading files and creating dataset. Let's use the nice utility we already have i.e. **sklearn.datasets**.\n",
        "\n",
        "We'll use it to load an image dataset i.e. **Olivetti Faces Dataset**. Task is simple, use this data to train KNN to classify these face images. And just to be clear we are talking about **KNN not K-Means**.\n",
        "\n",
        "One thing that may bother you is how do we train an ML algorithm to classify images? Well images are numbers right? So you flatten those images and each column will now represent the pixel value at the given position. Kinda captures the image info but it's not the time to dewel into that."
      ],
      "metadata": {
        "id": "GfoUoJh6UWGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Load Olivetti Faces Dataset using sklearn.datasets**"
      ],
      "metadata": {
        "id": "Mk47ziMDF3bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "d03kP9JmUbO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Split the dataset into training and testing set with 8:2 train-to-test such that the ratio of number of datapoints in all classes is same in both.**"
      ],
      "metadata": {
        "id": "ZPWMwGzQG-2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "xDsNQ2c_HXou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Train and Test the KNN model and find optimal k-value for the given test set.**"
      ],
      "metadata": {
        "id": "gLWW6bImHYz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "6jyGtH-CHnVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Why is KNN suitable for this task?**\n",
        "\n",
        "**Ans.**"
      ],
      "metadata": {
        "id": "lttH2lT6HpGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5[BONUS]. Given that data for each class is less use K-Folds to train and validate the model.**"
      ],
      "metadata": {
        "id": "9-4yhONpIKIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "vvefRQEgIf1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification with SVM\n",
        "\n",
        "Hurray! You just create a face recognition model maybe this isn't as good as Apples's but well you create it yourself, so it's better. Now that we have the foundations sorted let's do something interesting. We'll work on a not-so-popular popular dataset i.e. **CIFAR-10** and no that wasn't a typo.\n",
        "\n",
        "![](https://i.ibb.co/HpsK2HK/download.png)\n",
        "\n",
        "Well we are gonna assume you do and if not you can watch this [awesome video](https://www.youtube.com/watch?v=Lpr__X8zuE8) by one of my favourite educator **Luis Serrano** on the same and for sklearn refer [these docs](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). Well you can go and read the resear...\n",
        "\n",
        ">**Herumb:** If I write that, it won't end well for me would it.\n",
        ">\n",
        ">**Antaripa:** Nope.\n",
        "\n",
        "Haha let's start with our next task i.e. **Image Classification on CIFAR-10**. There is other interesting stuff but let's do things the normal way first."
      ],
      "metadata": {
        "id": "ggFbnmbjUeHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Download and load CIFAR-10 dataset using [`tf.keras.datasets`](tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data). Apply any preprocessing you feel necessary.** "
      ],
      "metadata": {
        "id": "fW_jQ9X6fnGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "a8AWHs6Clkql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Train an SVM classifier to classify the dataset.**"
      ],
      "metadata": {
        "id": "DeOraLhxheV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "K0od7pBRhd4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Test the classifier on `data_batch_5` testing set.**"
      ],
      "metadata": {
        "id": "Ust9Q5DEhyZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "q3jISDM_hUcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4[BONUS]. Use `GridSearchCV` or `RandomizedSearchCV` to tune the model.**"
      ],
      "metadata": {
        "id": "x-NOnjC5h_ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "QMrb8nIdidgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimentionality reduction\n",
        "\n",
        "As you can see the dataset is quite big and bruh does it take time to train this. One thing you can try is to apply dimentionality reduction using **PCA** or **T-SNE**. For this assignment we'll use PCA and if you don't know about PCA you can refer [this video](https://www.youtube.com/watch?v=FgakZw6K1QQ) or [this article](https://builtin.com/data-science/step-step-explanation-principal-component-analysis), and to know how to use it you can read [this article](https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python)."
      ],
      "metadata": {
        "id": "nLkSlI64ifXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. The task are same train the SVM classifier on CIFAR-10 except this time you are gonna use PCA to reduce the dimensions and check how it fares with the previous one.**"
      ],
      "metadata": {
        "id": "0CXEyLqel0hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "oKPdKEDTl9bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n",
        "\n",
        "Until now we mostly saw how we can use the image as it is to classify them. But we can also use something called as **Feature Descriptors**, to train the model. Some of the popular ones are **HOG** and **SIFT**. But why do we need them?\n",
        "\n",
        "Think like this, do we need all the info present in the image to classify it? If I have a football image and if I just have an outline of the football, can't I still predict that it's a football? You can right? \n",
        "\n",
        "Feature Descriptors work on the principle of keep useful info and removing any useless info in the image. For this assignment we'll limit ourselves to HOG only. To get HOG Features you can use [`skimage.feature.hog`](https://scikit-image.org/docs/dev/api/skimage.feature.html?highlight=hog#skimage.feature.hog)."
      ],
      "metadata": {
        "id": "rjxEJ7sSmBPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Your task is to apply HOG on CIFAR-10 and use them to train and test the SVM model to see how it fares with previous ones.**"
      ],
      "metadata": {
        "id": "iBFc7ZGso4sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE"
      ],
      "metadata": {
        "id": "EyrsGwtjvuOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Explain about Feature Descriptors and working of HOG.**\n",
        "\n",
        "**Ans.**"
      ],
      "metadata": {
        "id": "1gj00zU4w2sh"
      }
    }
  ]
}